{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxp1xzc_VCjn"
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-UJ-q4rUhoK"
      },
      "source": [
        "# Exploring Pretrained Models from PyTorch Hub\n",
        "\n",
        "This notebook explores various pretrained computer vision models available through PyTorch Hub. We'll cover:\n",
        "\n",
        "- **Object detection models** (SSD, YOLOv5)\n",
        "- **Instance segmentation models** (Mask R-CNN)\n",
        "\n",
        "## What is PyTorch Hub?\n",
        "\n",
        "PyTorch Hub is a pre-trained model repository designed to facilitate research reproducibility and enable easy access to state-of-the-art models. Models are loaded directly from GitHub repositories with a single line of code.\n",
        "\n",
        "## Datasets:\n",
        "\n",
        "- **ImageNet1k**: 1.2M images, 1000 classes (ILSVRC 2012)\n",
        "- **ImageNet21k**: 14M images, 21,841 classes (full ImageNet dataset)\n",
        "- **COCO**: Common Objects in Context, 330K images, 80 object categories (detection & segmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRe4pUaGUhoN"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4WtAc-nUhoO"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import Polygon\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from scipy.ndimage import zoom\n",
        "import cv2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSG1ZYKUhoP"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw3VOr5hUhoP"
      },
      "outputs": [],
      "source": [
        "def load_image_from_url(url):\n",
        "    \"\"\"\n",
        "    Load an image from a URL.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    return img\n",
        "\n",
        "\n",
        "def display_image(img, title=\"Image\", figsize=(8, 8)):\n",
        "    \"\"\"\n",
        "    Display an image with matplotlib.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_imagenet_labels():\n",
        "    \"\"\"\n",
        "    Load ImageNet class labels.\n",
        "    \"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "    response = requests.get(url)\n",
        "    labels = response.text.strip().split('\\n')\n",
        "    return labels\n",
        "\n",
        "\n",
        "def get_coco_labels():\n",
        "    \"\"\"\n",
        "    COCO dataset class labels (80 classes).\n",
        "    \"\"\"\n",
        "    return [\n",
        "        '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "        'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "        'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "        'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "        'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "        'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "        'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "    ]\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Count total and trainable parameters in a model.\n",
        "    \"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Size (MB): {total_params * 4 / (1024**2):.2f}\")  # Assuming float32\n",
        "\n",
        "    return total_params, trainable_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aevk-dfEUhoP"
      },
      "source": [
        "---\n",
        "# Part 2: Object Detection Models\n",
        "\n",
        "Object detection involves both locating objects (bounding boxes) and classifying them.\n",
        "\n",
        "## 2.1 SSD (Single Shot MultiBox Detector)\n",
        "\n",
        "### Overview\n",
        "SSD is a fast object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location.\n",
        "\n",
        "### Key Features:\n",
        "- Single-stage detector (no region proposals)\n",
        "- Multi-scale feature maps for detection\n",
        "- Fast inference speed\n",
        "- Uses VGG16 as backbone\n",
        "- Trained on COCO dataset (80 classes)\n",
        "\n",
        "**Paper:** *SSD: Single Shot MultiBox Detector*  \n",
        "Liu, W., et al. (2016)  \n",
        "[ECCV 2016 Paper](https://arxiv.org/abs/1512.02325)\n",
        "\n",
        "### Architecture:\n",
        "![SSD Architecture](https://miro.medium.com/v2/resize:fit:1400/1*51joMGlhxvftTxGtA4lA7Q.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnt0y-UbUhoQ"
      },
      "outputs": [],
      "source": [
        "# Load SSD model from torch.hub\n",
        "print(\"Loading SSD300 with VGG16 backbone...\")\n",
        "model_ssd = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', pretrained=True)\n",
        "model_ssd = model_ssd.to(device)\n",
        "model_ssd.eval()\n",
        "\n",
        "print(\"\\nSSD model loaded successfully!\")\n",
        "print(\"Model trained on COCO dataset (80 object classes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLYkTq2oUhoQ"
      },
      "outputs": [],
      "source": [
        "# Load image for object detection\n",
        "detection_url = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
        "img_detect = load_image_from_url(detection_url)\n",
        "display_image(img_detect, \"Image for Object Detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADaqqFzzUhoQ"
      },
      "outputs": [],
      "source": [
        "# Prepare image for SSD\n",
        "def prepare_ssd_input(image):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((300, 300)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "ssd_input = prepare_ssd_input(img_detect).to(device)\n",
        "\n",
        "# Get predictions\n",
        "utils_ssd = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')\n",
        "\n",
        "with torch.no_grad():\n",
        "    detections = model_ssd(ssd_input)\n",
        "\n",
        "# Process detections\n",
        "results_per_input = utils_ssd.decode_results(detections)\n",
        "best_results_per_input = [utils_ssd.pick_best(results, 0.4) for results in results_per_input]\n",
        "\n",
        "# Get COCO labels\n",
        "coco_labels = get_coco_labels()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SSD DETECTIONS (Confidence > 40%)\")\n",
        "print(\"=\"*60)\n",
        "for image_idx in range(len(best_results_per_input)):\n",
        "    bboxes, classes, confidences = best_results_per_input[image_idx]\n",
        "    for bbox, cls, conf in zip(bboxes, classes, confidences):\n",
        "        print(f\"Class: {coco_labels[cls]:20s} | Confidence: {conf:.2%} | BBox: {bbox}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivaUL2UnUhoQ"
      },
      "outputs": [],
      "source": [
        "# Visualize SSD detections\n",
        "def visualize_detections(image, bboxes, classes, confidences, labels, title=\"Detections\"):\n",
        "    \"\"\"\n",
        "    Visualize object detection results.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Generate colors\n",
        "    np.random.seed(42)\n",
        "    colors = np.random.rand(len(labels), 3)\n",
        "\n",
        "    img_width, img_height = image.size\n",
        "\n",
        "    for bbox, cls, conf in zip(bboxes, classes, confidences):\n",
        "        # SSD outputs normalized coordinates [xmin, ymin, xmax, ymax]\n",
        "        xmin = bbox[0] * img_width\n",
        "        ymin = bbox[1] * img_height\n",
        "        xmax = bbox[2] * img_width\n",
        "        ymax = bbox[3] * img_height\n",
        "\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        # Draw rectangle\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height,\n",
        "                                linewidth=2, edgecolor=colors[cls], facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add label\n",
        "        label_text = f\"{labels[cls]}: {conf:.2f}\"\n",
        "        ax.text(xmin, ymin - 5, label_text,\n",
        "               bbox=dict(boxstyle='round', facecolor=colors[cls], alpha=0.7),\n",
        "               fontsize=10, color='white', weight='bold')\n",
        "\n",
        "    ax.axis('off')\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize\n",
        "for image_idx in range(len(best_results_per_input)):\n",
        "    bboxes, classes, confidences = best_results_per_input[image_idx]\n",
        "    visualize_detections(img_detect, bboxes, classes, confidences,\n",
        "                        coco_labels, \"SSD Object Detection Results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sp0PF7rUhoR"
      },
      "source": [
        "## 2.2 YOLOv5 (You Only Look Once v5)\n",
        "\n",
        "### Overview\n",
        "YOLOv5 is one of the most popular object detection models, known for its speed and accuracy. It's widely used in real-time applications.\n",
        "\n",
        "### Key Features:\n",
        "- Single-stage detector\n",
        "- Extremely fast (up to 140 FPS on GPU)\n",
        "- Multiple model sizes (nano, small, medium, large, xlarge)\n",
        "- CSPDarknet53 backbone\n",
        "- Trained on COCO dataset\n",
        "- Easy to use and deploy\n",
        "\n",
        "**Repository:** [Ultralytics YOLOv5](https://github.com/ultralytics/yolov5)\n",
        "\n",
        "### Architecture:\n",
        "![YOLOv5 Architecture](https://user-images.githubusercontent.com/26456083/86477109-5a7ca780-bd7a-11ea-9cb7-48d9fd6848e7.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REzB9QiqUhoR"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv5 from torch.hub\n",
        "print(\"Loading YOLOv5s (small) model...\")\n",
        "model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model_yolo = model_yolo.to(device)\n",
        "model_yolo.eval()\n",
        "\n",
        "print(\"\\nYOLOv5s model loaded successfully!\")\n",
        "print(f\"Number of classes: {len(model_yolo.names)}\")\n",
        "print(f\"Model size: YOLOv5s (small)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vrZhNpaUhoR"
      },
      "outputs": [],
      "source": [
        "# Run inference with YOLOv5 (it handles preprocessing internally)\n",
        "results = model_yolo(img_detect)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"YOLOv5 DETECTIONS\")\n",
        "print(\"=\"*60)\n",
        "print(results.pandas().xyxy[0])  # Pandas DataFrame format\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5vprMm3UhoR"
      },
      "outputs": [],
      "source": [
        "# Visualize YOLOv5 results (built-in visualization)\n",
        "results.show()  # This will display the image with bounding boxes\n",
        "\n",
        "# Alternative: render as matplotlib\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.imshow(np.array(results.render()[0]))\n",
        "plt.axis('off')\n",
        "plt.title('YOLOv5 Object Detection Results', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAfrDgOFUhoR"
      },
      "outputs": [],
      "source": [
        "# Test YOLOv5 on a more complex scene\n",
        "complex_scene_url = \"https://images.unsplash.com/photo-1721910256794-fb3a7896ba45?fm=jpg&q=60&w=3000&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1yZWxhdGVkfDE5fHx8ZW58MHx8fHx8\"\n",
        "img_complex = load_image_from_url(complex_scene_url)\n",
        "display_image(img_complex, \"Complex Scene for Detection\")\n",
        "\n",
        "# Run detection\n",
        "results_complex = model_yolo(img_complex)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.imshow(np.array(results_complex.render()[0]))\n",
        "plt.axis('off')\n",
        "plt.title('YOLOv5 Detection on Complex Scene', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detections\n",
        "print(\"\\nDetected objects:\")\n",
        "print(results_complex.pandas().xyxy[0][['name', 'confidence']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQuxzBjJUhoS"
      },
      "outputs": [],
      "source": [
        "results_complex.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfJsm-kPUhoS"
      },
      "source": [
        "---\n",
        "# Part 3: Instance Segmentation\n",
        "\n",
        "## 3.1 Mask R-CNN ResNet50 FPN v2\n",
        "\n",
        "### Overview\n",
        "Mask R-CNN extends Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI). It performs:\n",
        "1. Object detection (bounding boxes + class labels)\n",
        "2. Instance segmentation (pixel-level masks for each object)\n",
        "\n",
        "### Key Features:\n",
        "- Two-stage detector\n",
        "- ResNet50 backbone with FPN (Feature Pyramid Network)\n",
        "- Predicts masks at pixel level\n",
        "- State-of-the-art instance segmentation\n",
        "- Trained on COCO dataset\n",
        "\n",
        "**Paper:** *Mask R-CNN*  \n",
        "He, K., Gkioxari, G., DollÃ¡r, P., & Girshick, R. (2017)  \n",
        "[ICCV 2017 Paper](https://arxiv.org/abs/1703.06870)\n",
        "\n",
        "### Architecture:\n",
        "![Mask R-CNN Architecture](https://cdn.prod.website-files.com/680a070c3b99253410dd3df5/68ee302b0bd7d85155faad63_684d854d2575826aacdbf2a1_67ed516491886a7a596e2fda_67dd6e0e3d3570fece529542_rcnn_fig4.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITVKFLBIUhoS"
      },
      "outputs": [],
      "source": [
        "# Load Mask R-CNN ResNet50 FPN v2\n",
        "print(\"Loading Mask R-CNN ResNet50 FPN v2...\")\n",
        "model_maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(pretrained=True)\n",
        "model_maskrcnn = model_maskrcnn.to(device)\n",
        "model_maskrcnn.eval()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MASK R-CNN RESNET50 FPN V2\")\n",
        "print(\"=\"*60)\n",
        "print(\"Model loaded successfully!\")\n",
        "print(\"Trained on COCO dataset (80 object classes)\")\n",
        "print(\"\\nParameter count:\")\n",
        "count_parameters(model_maskrcnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxG8XMZiUhoS"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "# Prepare image for Mask R-CNN\n",
        "transform_maskrcnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load image for segmentation\n",
        "segmentation_url = \"https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\"\n",
        "img_segment = load_image_from_url(segmentation_url)\n",
        "display_image(img_segment, \"Image for Instance Segmentation\")\n",
        "\n",
        "# Transform image\n",
        "img_tensor = transform_maskrcnn(img_segment).to(device)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    predictions = model_maskrcnn([img_tensor])[0]\n",
        "\n",
        "# Print predictions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MASK R-CNN PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Number of detected instances: {len(predictions['labels'])}\")\n",
        "print(\"\\nTop detections (confidence > 0.5):\")\n",
        "\n",
        "for i, (label, score, box) in enumerate(zip(predictions['labels'], predictions['scores'], predictions['boxes'])):\n",
        "    if score > 0.5:\n",
        "        print(f\"{i+1}. {coco_labels[label]:20s} | Confidence: {score:.2%} | Box: [{box[0]:.1f}, {box[1]:.1f}, {box[2]:.1f}, {box[3]:.1f}]\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "P9eHtuJ1UhoS",
        "outputId": "224bf325-9a92-4c8f-a882-8e2592d1fbe0"
      },
      "outputs": [],
      "source": [
        "# Visualize instance segmentation results with polygon contours\n",
        "def visualize_instance_segmentation(image, predictions, labels, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Visualize instance segmentation with polygon masks and bounding boxes.\n",
        "    Converts masks to polygon contours for cleaner visualization.\n",
        "    \"\"\"\n",
        "    # Filter predictions by threshold first\n",
        "    mask_indices = [i for i, score in enumerate(predictions['scores']) if score > threshold]\n",
        "\n",
        "    if len(mask_indices) == 0:\n",
        "        print(f\"No detections found with confidence > {threshold}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Visualizing {len(mask_indices)} detections with confidence > {threshold}...\")\n",
        "\n",
        "    # Convert image to numpy array\n",
        "    if isinstance(image, Image.Image):\n",
        "        img_array = np.array(image)\n",
        "    else:\n",
        "        img_array = image\n",
        "\n",
        "    # Create figure with 3 subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "\n",
        "    # Subplot 1: Original image\n",
        "    axes[0].imshow(img_array)\n",
        "    axes[0].set_title('Original Image', fontsize=16, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Subplot 2: Image with bounding boxes\n",
        "    axes[1].imshow(img_array)\n",
        "    axes[1].set_title(f'Bounding Boxes ({len(mask_indices)} detections)', fontsize=16, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Subplot 3: Image with transparent polygon masks\n",
        "    axes[2].imshow(img_array)\n",
        "    axes[2].set_title('Instance Segmentation Masks', fontsize=16, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Generate random colors for each unique class\n",
        "    np.random.seed(42)\n",
        "    max_label = max([predictions['labels'][idx].item() for idx in mask_indices])\n",
        "    colors = np.random.rand(max_label + 1, 3)\n",
        "\n",
        "    # Process each detection\n",
        "    for idx in mask_indices:\n",
        "        label = predictions['labels'][idx].item()\n",
        "        score = predictions['scores'][idx].item()\n",
        "        box = predictions['boxes'][idx].cpu().numpy()\n",
        "        mask = predictions['masks'][idx, 0].cpu().numpy()\n",
        "\n",
        "        # Get box coordinates\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        # Draw bounding box on subplot 2\n",
        "        rect = patches.Rectangle((xmin, ymin), width, height,\n",
        "                                linewidth=3, edgecolor=colors[label], facecolor='none')\n",
        "        axes[1].add_patch(rect)\n",
        "\n",
        "        # Add label text on subplot 2\n",
        "        label_text = f\"{labels[label]}: {score:.2f}\"\n",
        "        axes[1].text(xmin, ymin - 10, label_text,\n",
        "                   bbox=dict(boxstyle='round', facecolor=colors[label], alpha=0.9),\n",
        "                   fontsize=12, color='white', weight='bold')\n",
        "\n",
        "        # Convert mask to polygon contours for subplot 3\n",
        "        # Resize mask to image dimensions if needed\n",
        "        if mask.shape != img_array.shape[:2]:\n",
        "            zoom_factors = (img_array.shape[0] / mask.shape[0],\n",
        "                          img_array.shape[1] / mask.shape[1])\n",
        "            mask = zoom(mask, zoom_factors, order=1)\n",
        "\n",
        "        # Create binary mask\n",
        "        mask_binary = (mask > 0.5).astype(np.uint8)\n",
        "\n",
        "        # Find contours using OpenCV\n",
        "        contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Draw each contour as a polygon\n",
        "        for contour in contours:\n",
        "            # Simplify contour to reduce points (optional, for smoother appearance)\n",
        "            epsilon = 0.005 * cv2.arcLength(contour, True)\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "\n",
        "            # Convert contour to polygon coordinates\n",
        "            if len(approx) >= 3:  # Need at least 3 points for a polygon\n",
        "                polygon_coords = approx.reshape(-1, 2)\n",
        "\n",
        "                # Create matplotlib Polygon patch with transparency\n",
        "                poly = Polygon(polygon_coords,\n",
        "                             facecolor=colors[label],\n",
        "                             edgecolor=colors[label],\n",
        "                             alpha=0.5,  # 50% transparency\n",
        "                             linewidth=2)\n",
        "                axes[2].add_patch(poly)\n",
        "\n",
        "        # Add bounding box on subplot 3\n",
        "        rect3 = patches.Rectangle((xmin, ymin), width, height,\n",
        "                                 linewidth=2, edgecolor=colors[label], facecolor='none')\n",
        "        axes[2].add_patch(rect3)\n",
        "\n",
        "        # Add label text on subplot 3\n",
        "        axes[2].text(xmin, ymin - 10, label_text,\n",
        "                    bbox=dict(boxstyle='round', facecolor=colors[label], alpha=0.9),\n",
        "                    fontsize=12, color='white', weight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detection summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"DETECTED {len(mask_indices)} INSTANCES (confidence > {threshold})\")\n",
        "    print(f\"{'='*70}\")\n",
        "    for idx in mask_indices:\n",
        "        label_id = predictions['labels'][idx].item()\n",
        "        score = predictions['scores'][idx].item()\n",
        "        box = predictions['boxes'][idx].cpu().numpy()\n",
        "        print(f\"  [{score:.2%}] {labels[label_id]:20s} @ [{box[0]:.0f}, {box[1]:.0f}, {box[2]:.0f}, {box[3]:.0f}]\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "# Visualize results\n",
        "visualize_instance_segmentation(img_segment, predictions, coco_labels, threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z5E8aVvpUhoS",
        "outputId": "ccfca7e8-787e-4793-e65b-c5f9591ee477"
      },
      "outputs": [],
      "source": [
        "# Test on a scene with multiple objects\n",
        "multi_object_url = \"https://images.unsplash.com/photo-1511688878353-3a2f5be94cd7?w=800\"\n",
        "img_multi = load_image_from_url(multi_object_url)\n",
        "display_image(img_multi, \"Multi-object Scene for Segmentation\")\n",
        "\n",
        "# Transform and predict\n",
        "img_multi_tensor = transform_maskrcnn(img_multi).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions_multi = model_maskrcnn([img_multi_tensor])[0]\n",
        "\n",
        "print(f\"\\nDetected {len([s for s in predictions_multi['scores'] if s > 0.5])} objects with confidence > 0.5\")\n",
        "\n",
        "# Visualize\n",
        "visualize_instance_segmentation(img_multi, predictions_multi, coco_labels, threshold=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQhd6NNpUhoT"
      },
      "source": [
        "---\n",
        "# Summary and Key Takeaways\n",
        "\n",
        "## What We Learned:\n",
        "\n",
        "### 1. Object Detection Models\n",
        "- **SSD**: Single-stage detector, good balance of speed and accuracy\n",
        "- **YOLOv5**: Fastest detection method, ideal for real-time applications\n",
        "- Both trained on **COCO dataset** with 80 object categories\n",
        "- Single-stage detectors are faster but may be less accurate than two-stage methods\n",
        "\n",
        "### 2. Instance Segmentation\n",
        "- **Mask R-CNN**: Extends object detection with pixel-level segmentation masks\n",
        "- Two-stage approach: first detects objects, then segments them\n",
        "- More computationally expensive but provides detailed object boundaries\n",
        "- Essential for applications requiring precise object localization\n",
        "\n",
        "## PyTorch Hub Benefits:\n",
        "1. **Easy model loading**: Single line of code to load pretrained models\n",
        "2. **Reproducibility**: Ensures consistent model versions\n",
        "3. **Community models**: Access to models from various repositories\n",
        "4. **No manual downloads**: Automatic model weight downloading\n",
        "\n",
        "## Model Trade-offs:\n",
        "- **Speed**: YOLOv5 > SSD > Mask R-CNN\n",
        "- **Accuracy**: Mask R-CNN > SSD > YOLOv5 (generally)\n",
        "- **Detail**: Mask R-CNN provides pixel-level masks, others only bounding boxes\n",
        "\n",
        "## Practical Applications:\n",
        "- **Object Detection**: Autonomous driving, surveillance, retail analytics, crowd counting\n",
        "- **Instance Segmentation**: Medical imaging, robotics, augmented reality, precise object manipulation\n",
        "\n",
        "## Next Steps:\n",
        "1. Fine-tune models on custom datasets\n",
        "2. Experiment with different YOLOv5 variants (n, s, m, l, x)\n",
        "3. Deploy models for production use\n",
        "4. Optimize for mobile/edge devices (ONNX, TensorRT)\n",
        "5. Explore more recent architectures (YOLOv8, YOLOv9, DETR, Swin Transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqabekE-UhoT"
      },
      "source": [
        "---\n",
        "## Additional Resources\n",
        "\n",
        "### Papers:\n",
        "1. **SSD**: Liu et al., \"SSD: Single Shot MultiBox Detector\" (ECCV 2016) - [arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)\n",
        "2. **YOLOv5**: Ultralytics - [GitHub Repository](https://github.com/ultralytics/yolov5)\n",
        "3. **Mask R-CNN**: He et al., \"Mask R-CNN\" (ICCV 2017) - [arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)\n",
        "\n",
        "### Datasets:\n",
        "- [COCO Dataset](https://cocodataset.org/) - 80 object categories for detection and segmentation\n",
        "- [ImageNet](https://www.image-net.org/) - 1000 classes (ImageNet1k) or 21,841 classes (ImageNet21k)\n",
        "- [Papers With Code - Datasets](https://paperswithcode.com/datasets)\n",
        "\n",
        "### Documentation:\n",
        "- [PyTorch Hub](https://pytorch.org/hub/) - Pre-trained model repository\n",
        "- [TorchVision Models](https://pytorch.org/vision/stable/models.html) - Official torchvision models\n",
        "- [Ultralytics YOLOv5 Docs](https://docs.ultralytics.com/yolov5/)\n",
        "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
        "\n",
        "### Model Repositories:\n",
        "- [NVIDIA Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples) - SSD and other optimized models\n",
        "- [Ultralytics YOLOv5](https://github.com/ultralytics/yolov5) - Most popular YOLO implementation\n",
        "- [Detectron2](https://github.com/facebookresearch/detectron2) - Facebook's detection and segmentation platform\n",
        "\n",
        "### Further Exploration:\n",
        "- Try different YOLOv5 model sizes: `yolov5n`, `yolov5s`, `yolov5m`, `yolov5l`, `yolov5x`\n",
        "- Explore YOLOv8 and YOLOv9 (latest versions with improved performance)\n",
        "- Learn about semantic segmentation (FCN, DeepLab, SegFormer)\n",
        "- Experiment with model quantization and pruning for deployment\n",
        "- Convert models to ONNX format for cross-platform inference\n",
        "- Use TensorRT for optimized GPU inference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "armasuisse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
